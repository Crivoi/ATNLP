{"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom enum import Enum\nimport re\nimport random","metadata":{"tags":[],"cell_id":"83b7e3824603486f915783312a5e2286","source_hash":"e00ace41","execution_start":1669901037338,"execution_millis":3391,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"/shared-libs/python3.9/py/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"SOS_token = 0\nEOS_token = 1\nOOV_token = 2","metadata":{"tags":[],"cell_id":"294663b3c7344931992b26760d7776b6","source_hash":"c303e84","execution_start":1669901040743,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataloading","metadata":{"tags":[],"cell_id":"4ee9f203bf9243de94455a54edef0270","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"}},{"cell_type":"code","source":"!git clone https://github.com/brendenlake/SCAN","metadata":{"tags":[],"cell_id":"fa88269641894e609d400d26f33cea87","source_hash":"5cbe3f70","execution_start":1669901040792,"execution_millis":736,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"fatal: destination path 'SCAN' already exists and is not an empty directory.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"class ScanSplit(Enum):\n    SIMPLE_SPLIT = 'simple_split'\n    LENGTH_SPLIT = 'length_split'\n    FEW_SHOT_SPLIT = 'few_shot_split'\n    ADD_PRIM_JUMP_SPLIT = 'add_prim_split'\n    ADD_PRIM_TURNLEFT_SPLIT = 'add_prim_split' # shouldn't have same value => if condition won't happen","metadata":{"tags":[],"cell_id":"2b7aa7a73ca84559940108166837424f","source_hash":"dd85ec7d","execution_start":1669901041110,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Lang:\n    def __init__(self):\n        self.word2index = {}\n        self.word2count = {}\n        self.index2word = {SOS_token: \"SOS\", EOS_token: \"EOS\", OOV_token: 'OOV'}\n        self.n_words = len(self.index2word)  # Count tokens\n\n        self.max_length = 0\n\n    def add_sentence(self, sentence):\n        \"\"\"Add sentence to vocab\"\"\"\n        for word in sentence.split(' '):\n            self._add_word(word)\n\n    def _add_word(self, word):\n        \"\"\"Add word to vocab\"\"\"\n        if word not in self.word2index:\n            self.word2index[word] = self.n_words\n            self.word2count[word] = 1\n            self.index2word[self.n_words] = word\n            self.n_words += 1\n            self.max_length = max(len(word), self.max_length)\n        else:\n            self.word2count[word] += 1\n\n    def indexes_from_sentence(self, sentence: str):\n        \"\"\"Get word ids from sentence\"\"\"\n        indexes = [self.word2index.get(word,OOV_token) for word in sentence.split(' ')]\n        return indexes\n\n    def tensor_from_sentence(self, sentence:str):\n        \"\"\"Convert sentence to torch tensor\"\"\"\n        indexes = self.indexes_from_sentence(sentence)\n        return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)","metadata":{"tags":[],"cell_id":"366795fa93c4426d8c7b9ec5f58a52e4","source_hash":"bb1dc3ed","execution_start":1669901041119,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ScanDataset(Dataset):\n    def __init__(self, split: ScanSplit, input_lang: Lang, output_lang: Lang, train: bool = True):\n        \n        self.input_lang = input_lang\n        self.output_lang = output_lang\n\n\n        self.X, self.y = self._get_data(split, train)\n\n\n    def __len__(self):\n            return len(self.y)\n\n    def __getitem__(self, idx):\n            input_tensor = self.input_lang.tensor_from_sentence(self.X[idx])\n            target_tensor = self.output_lang.tensor_from_sentence(self.y[idx])\n            return (input_tensor, target_tensor)\n    \n\n    def _get_data(self, split: ScanSplit, train: bool = True):\n        \"\"\"Retrieve the right data for the selected split\"\"\"\n        \n        if split == ScanSplit.SIMPLE_SPLIT:\n            X_train, y_train = self._extract_data_from_file('SCAN/simple_split/tasks_train_simple.txt')\n            X_test, y_test = self._extract_data_from_file('SCAN/simple_split/tasks_test_simple.txt')\n        elif split == ScanSplit.LENGTH_SPLIT:\n            X_train, y_train = self._extract_data_from_file('SCAN/length_split/tasks_train_length.txt')\n            X_test, y_test = self._extract_data_from_file('SCAN/length_split/tasks_test_length.txt')\n        elif split == ScanSplit.ADD_PRIM_JUMP_SPLIT:\n            X_train, y_train = self._extract_data_from_file('SCAN/add_prim_split/tasks_train_addprim_jump.txt')\n            X_test, y_test = self._extract_data_from_file('SCAN/add_prim_split/tasks_test_addprim_jump.txt')\n        elif split == ScanSplit.ADD_PRIM_TURNLEFT_SPLIT:\n            X_train, y_train = self._extract_data_from_file('SCAN/add_prim_split/tasks_train_addprim_turn_left.txt')\n            X_test, y_test = self._extract_data_from_file('SCAN/add_prim_split/tasks_test_addprim_turn_left.txt')\n        else:\n            raise Exception('Split not implemented')\n        \n        if train:\n            X = X_train\n            y = y_train\n\n            # Add words to vocabs\n            for sen in X:\n                self.input_lang.add_sentence(sen)\n\n            for sen in y:\n                self.output_lang.add_sentence(sen)\n        else:\n            X = X_test\n            y = y_test\n\n        return X,y\n        \n    def _extract_data_from_file(self, filepath: str):\n        \"\"\"Get X and y from SCAN file\"\"\"\n        with open(filepath) as f:\n            txt_data = f.readlines()\n\n        # Format is in IN: ... OUT: ...\n        lead_token = 'IN:'\n        split_token = 'OUT:'\n\n        # Split at OUT and remove IN\n        txt_data = [sen.strip(lead_token).split(split_token) for sen in txt_data]\n\n        in_txt = [sen[0] for sen in txt_data]\n        out_txt = [sen[1] for sen in txt_data]\n\n        return in_txt, out_txt","metadata":{"tags":[],"cell_id":"e92159ab22334d2abc2cbfe0df64ec2d","source_hash":"b8448b5a","execution_start":1669901041128,"execution_millis":7,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\nis_cuda = torch.cuda.is_available()\n\n# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\nif is_cuda:\n    device = torch.device(\"cuda\")\n    print(\"GPU is available\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"GPU not available, CPU used\")","metadata":{"tags":[],"cell_id":"1a6ef5742e474040961a7cad0877eeeb","source_hash":"3c8963ba","execution_start":1669901041137,"execution_millis":393,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"GPU not available, CPU used\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"train_dataset = ScanDataset(\n    split=ScanSplit.SIMPLE_SPLIT,\n    input_lang=Lang(),\n    output_lang=Lang(),\n    train=True\n    )\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n\nMAX_LENGTH = max(train_dataset.input_lang.max_length, train_dataset.output_lang.max_length)","metadata":{"tags":[],"cell_id":"5600ac8184e74209a7957f4a8c7d8e47","source_hash":"d8e50637","execution_start":1669901042628,"execution_millis":499,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset = ScanDataset(\n    split=ScanSplit.SIMPLE_SPLIT,\n    input_lang=Lang(),\n    output_lang=Lang(),\n    train=False\n)","metadata":{"tags":[],"cell_id":"89323afc18124e0b93067aaa829745bd","source_hash":"d3898469","execution_start":1669901391847,"execution_millis":57,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model","metadata":{"tags":[],"cell_id":"f21e61d2351b4e59a63adf1629de7b4c","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"}},{"cell_type":"code","source":"class EncoderRNN(nn.Module):\n    def __init__(self, vocab_size, embedding_dim):\n        super(EncoderRNN, self).__init__()\n\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.rnn = nn.RNN(embedding_dim, embedding_dim)\n\n    def forward(self, x):\n\n        embeds = self.embedding(x)\n        _, hidden = self.rnn(embeds)\n    \n        return hidden.squeeze()","metadata":{"tags":[],"cell_id":"213cfe79ef3f45a5811192f7185701d4","source_hash":"b0f92d32","execution_start":1669304032589,"execution_millis":0,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DecoderCell(nn.Module):\n    def __init__(self, vocab_size, embedding_dim):\n        super(DecoderCell, self).__init__()\n        self.rnn = nn.RNN(1, embedding_dim)\n        self.W = torch.nn.Parameter(torch.randn((embedding_dim, embedding_dim)))\n        self.U = torch.nn.Parameter(torch.randn((embedding_dim, embedding_dim)))\n        self.v = torch.nn.Parameter(torch.randn((embedding_dim, 1)))\n        self.nonlinear = torch.nn.Tanh()\n\n    def e(self, g, h):\n        h = torch.tensor([h]).unsqueeze(1)\n        return self.v.T @ self.nonlinear(self.W * g + self.U * h)\n\n    def alpha(self, encoder_hiddens, input_hidden, t):\n        T = len(encoder_hiddens)\n        top = torch.exp(self.e(input_hidden, encoder_hiddens[t]))\n\n        bottom = 0\n\n        for j in range(T):\n            bottom += torch.exp(self.e(input_hidden, encoder_hiddens[j]))\n\n        return top/bottom\n\n    def forward(self, x, encoder_hiddens, input_hidden):\n        c_i = 0\n\n        for t in range(len(encoder_hiddens)):\n            alpha_it = self.alpha(encoder_hiddens, input_hidden, t)\n            h_t = encoder_hiddens[t]\n            c_i += alpha_it * h_t\n\n        _, hidden = self.rnn(x, c_i)\n        hidden = torch.concat((hidden, c_i), dim=1).squeeze()\n        prediction = torch.argmax(F.softmax(hidden, dim=0))\n\n        return prediction, hidden\n\nclass DecoderRNN(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, comm_to_ix, act_to_ix):\n        super(DecoderRNN, self).__init__()\n        self.decoder_cell = DecoderCell(vocab_size, embedding_dim)\n        self.comm_to_ix = comm_to_ix\n        self.act_to_ix = act_to_ix\n\n    def forward(self, encoder_hiddens, use_teacher_forcing, targets):        \n        preds = []\n\n        if not use_teacher_forcing:\n            x = torch.tensor(self.comm_to_ix[\"<SOS>\"], dtype=torch.long)\n            pred, hiddens = self.decoder_cell(x.reshape(1, -1).float(), encoder_hiddens, encoder_hiddens[-1])\n            preds.append(pred)\n\n            while preds[-1].item() != self.act_to_ix[\"<EOS>\"]:\n                pred, hiddens = self.decoder_cell(pred.reshape(1, -1).float(), encoder_hiddens, hiddens[-1])\n                preds.append(pred)\n        else:\n            hiddens = encoder_hiddens\n\n            for x in targets:\n                pred, hiddens = self.decoder_cell(x.reshape(1, -1).float(), encoder_hiddens, hiddens[-1])\n                preds.append(pred)\n\n        return preds","metadata":{"tags":[],"cell_id":"83059e94df2742399adffea6bce822fc","source_hash":"8e184cb6","execution_start":1669304032589,"execution_millis":1,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, comm_to_ix, act_to_ix):\n        super(Model, self).__init__()\n        self.encoder = EncoderRNN(vocab_size, embedding_dim)\n        self.decoder = DecoderRNN(vocab_size, embedding_dim, comm_to_ix, act_to_ix)\n\n    def forward(self, x, use_teacher_forcing=False, targets=None):\n        assert (not use_teacher_forcing and targets is None) or (use_teacher_forcing and targets is not None)\n\n        encoder_hiddens = self.encoder(x)\n        preds = self.decoder(encoder_hiddens, use_teacher_forcing, targets)\n\n        return preds","metadata":{"tags":[],"cell_id":"7da7ead5ea3b478aaa87e4dc59980821","source_hash":"a68cdb3c","execution_start":1669305229052,"execution_millis":1,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\" EXAMPLE ON HOW TO USE THE SEQ2SEQ-model \"\"\"\n\n# Defining and encoding the input command\ncommand = \"jump twice and walk\".split()\ncomm_to_ix = {\"<SOS>\" : 0 , \"<EOS>\" : 1}\n\nfor word in command:\n    if word not in comm_to_ix:\n        comm_to_ix[word] = len(comm_to_ix)\n\n# Defining and encoding the target output action sequence\naction_sequence = \"JUMP JUMP WALK\".split()\nact_to_ix = {\"<SOS>\" : 0 , \"<EOS>\" : 1}\n\nfor act in action_sequence:\n    if act not in act_to_ix:\n        act_to_ix[act] = len(act_to_ix)\n\n# Preparing the input command and target output action sequence\ndef prepare_sequence(seq, to_ix):\n    seq = [\"<SOS>\"] + seq + [\"<EOS>\"]\n    idxs = [to_ix[w] for w in seq]\n    return torch.tensor(idxs, dtype=torch.long)\n\ncommand_prep = prepare_sequence(command, comm_to_ix)\naction_seq_prep = prepare_sequence(action_sequence, act_to_ix)\n\n# Some parameters\nVOCAB_SIZE = len(command_prep)\nEMBEDDING_DIM = 6 # Can be changed. In the paper they use 256\n\n# Defining the model and using it for predicting\nmodel = Model(VOCAB_SIZE, EMBEDDING_DIM, comm_to_ix, act_to_ix)\npred = model(command_prep, use_teacher_forcing=True, targets=action_seq_prep)","metadata":{"tags":[],"cell_id":"064c93e57e42416c861ccf66870c381c","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"teacher_forcing_ratio = 0.5\n\n\ndef train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n    encoder_hidden = encoder.initHidden()\n\n    encoder_optimizer.zero_grad()\n    decoder_optimizer.zero_grad()\n\n    input_length = input_tensor.size(0)\n    target_length = target_tensor.size(0)\n\n    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n\n    loss = 0\n\n    for ei in range(input_length):\n        encoder_output, encoder_hidden = encoder(\n            input_tensor[ei], encoder_hidden)\n        encoder_outputs[ei] = encoder_output[0, 0]\n\n    decoder_input = torch.tensor([[SOS_token]], device=device)\n\n    decoder_hidden = encoder_hidden\n\n    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n\n    if use_teacher_forcing:\n        # Teacher forcing: Feed the target as the next input\n        for di in range(target_length):\n            decoder_output, decoder_hidden, decoder_attention = decoder(\n                decoder_input, decoder_hidden, encoder_outputs)\n            loss += criterion(decoder_output, target_tensor[di])\n            decoder_input = target_tensor[di]  # Teacher forcing\n\n    else:\n        # Without teacher forcing: use its own predictions as the next input\n        for di in range(target_length):\n            decoder_output, decoder_hidden, decoder_attention = decoder(\n                decoder_input, decoder_hidden, encoder_outputs)\n            topv, topi = decoder_output.topk(1)\n            decoder_input = topi.squeeze().detach()  # detach from history as input\n\n            loss += criterion(decoder_output, target_tensor[di])\n            if decoder_input.item() == EOS_token:\n                break\n\n    loss.backward()\n\n    encoder_optimizer.step()\n    decoder_optimizer.step()\n\n    return loss.item() / target_length","metadata":{"tags":[],"cell_id":"f950985c9ccb4b9aa128cfcb4c7a806c","source_hash":"96159c70","execution_start":1669305277394,"execution_millis":0,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nimport math\n\n\ndef asMinutes(s):\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s / (percent)\n    rs = es - s\n    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))","metadata":{"tags":[],"cell_id":"5c2d6545d0f24803bf672380245c0327","source_hash":"af6e246d","execution_start":1669304032592,"execution_millis":4,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.switch_backend('agg')\nimport matplotlib.ticker as ticker\nimport numpy as np\n\n\ndef showPlot(points):\n    plt.figure()\n    fig, ax = plt.subplots()\n    # this locator puts ticks at regular intervals\n    loc = ticker.MultipleLocator(base=0.2)\n    ax.yaxis.set_major_locator(loc)\n    plt.plot(points)","metadata":{"tags":[],"cell_id":"5d783cbbad6740d6b03668d1ceb82aae","source_hash":"7321f3c1","execution_start":1669304032599,"execution_millis":8,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n    start = time.time()\n    plot_losses = []\n    print_loss_total = 0  # Reset every print_every\n    plot_loss_total = 0  # Reset every plot_every\n\n    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n    criterion = nn.NLLLoss()\n\n    for iter in range(1, n_iters + 1):\n        input_tensor, target_tensor = train_dataset[random.randrange(len(train_dataset))]\n\n        loss = train(input_tensor,target_tensor,encoder,decoder,encoder_optimizer,decoder_optimizer,criterion)\n        print_loss_total += loss\n        plot_loss_total += loss\n\n        if iter % print_every == 0:\n            print_loss_avg = print_loss_total / print_every\n            print_loss_total = 0\n            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n                                         iter, iter / n_iters * 100, print_loss_avg))\n\n        if iter % plot_every == 0:\n            plot_loss_avg = plot_loss_total / plot_every\n            plot_losses.append(plot_loss_avg)\n            plot_loss_total = 0\n\n    showPlot(plot_losses)","metadata":{"tags":[],"cell_id":"983f43609c8744a4a934f0ae9f41bd3f","source_hash":"1af869af","execution_start":1669305246510,"execution_millis":1,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate(encoder, decoder, sentence, max_length):\n    with torch.no_grad():\n        input_tensor = tensorFromSentence(input_lang, sentence)\n        input_length = input_tensor.size()[0]\n        encoder_hidden = encoder.initHidden()\n\n        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n\n        for ei in range(input_length):\n            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n                                                     encoder_hidden)\n            encoder_outputs[ei] += encoder_output[0, 0]\n\n        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n\n        decoder_hidden = encoder_hidden\n\n        decoded_words = []\n        decoder_attentions = torch.zeros(max_length, max_length)\n\n        for di in range(max_length):\n            decoder_output, decoder_hidden, decoder_attention = decoder(\n                decoder_input, decoder_hidden, encoder_outputs)\n            decoder_attentions[di] = decoder_attention.data\n            topv, topi = decoder_output.data.topk(1)\n            if topi.item() == EOS_token:\n                decoded_words.append('<EOS>')\n                break\n            else:\n                decoded_words.append(output_lang.index2word[topi.item()])\n\n            decoder_input = topi.squeeze().detach()\n\n        return decoded_words, decoder_attentions[:di + 1]","metadata":{"tags":[],"cell_id":"d61a224f702248ffac3368bce3e70d3d","source_hash":"37971e1e","execution_start":1669304032632,"execution_millis":4,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hidden_size = 256\nencoder1 = EncoderRNN(train_dataset.input_lang.n_words, hidden_size).to(device)\ndecoder1 = AttnDecoderRNN(hidden_size, train_dataset.output_lang.n_words, train_dataset.output_lang.max_length).to(device)\n\ntrainIters(encoder1, decoder1, 75000, print_every=5000)","metadata":{"tags":[],"cell_id":"7c29ee32b02340ab8ae5c654ae7be577","source_hash":"682e3251","execution_start":1669305280663,"execution_millis":4015578,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"4m 28s (- 62m 35s) (5000 6%) 1.1104\n9m 15s (- 60m 12s) (10000 13%) 0.3982\n13m 53s (- 55m 34s) (15000 20%) 0.1921\n18m 37s (- 51m 12s) (20000 26%) 0.1191\n23m 37s (- 47m 15s) (25000 33%) 0.1643\n28m 38s (- 42m 57s) (30000 40%) 0.1135\n33m 32s (- 38m 19s) (35000 46%) 0.2540\n38m 27s (- 33m 39s) (40000 53%) 2.0510\n42m 55s (- 28m 36s) (45000 60%) 4.1028\n47m 24s (- 23m 42s) (50000 66%) 5.9702\n51m 26s (- 18m 42s) (55000 73%) 5.8151\n55m 11s (- 13m 47s) (60000 80%) 5.4395\n59m 8s (- 9m 5s) (65000 86%) 5.5666\n63m 12s (- 4m 30s) (70000 93%) 5.6084\n66m 55s (- 0m 0s) (75000 100%) 5.5129\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"### Experiment 1","metadata":{"tags":[],"cell_id":"4fda507716e04e1da5a422df666bb646","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"markdown","source":"The top-performing architecture was a LSTM with no attention, 2\r\nlayers of 200 hidden units, and no dropout. The best-overall\r\nnetwork achieved 99.7% correct.","metadata":{"tags":[],"cell_id":"c14a4cb13a854d95aaea5da9032d53a7","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"SCAN tasks were randomly split into a training set (80%) and a test set (20%).","metadata":{"tags":[],"cell_id":"c85bcb5e3c8b444a98c2b846a36172c4","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"","metadata":{"tags":[],"cell_id":"ed96f94f1e5c42b19a496dcc187ead49","source_hash":"b623e53d","deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Experiment 2","metadata":{"tags":[],"cell_id":"b851b543e9554502bc57f59516fc5b5b","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"markdown","source":"The best result (20.8% on average, again over 5 runs) is achieved\r\nby a GRU with attention, one 50-dimensional hidden layer,\r\nand dropout 0.5","metadata":{"tags":[],"cell_id":"9c0aaa4bc00b46ca87b3501f21ceaa9a","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"","metadata":{"tags":[],"cell_id":"3b298d8ea5b0467c9268fded8b4dbee5","source_hash":"b623e53d","deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Experiment 3","metadata":{"tags":[],"cell_id":"1a94160769b845c3ba78b7ad9983f21f","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"markdown","source":"The best performance is achieved by\r\na GRU network with attention, one layer with 100 hidden\r\nunits, and dropout of 0.1 (90.3% accuracy). ","metadata":{"tags":[],"cell_id":"cfa2057430514406a378bb52288a8e12","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"","metadata":{"tags":[],"cell_id":"095b32d0fce7491b884bf95e3ce2a3e4","source_hash":"b623e53d","deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=ec00d141-8917-4313-a10a-78395d2ec852' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"d970c84276ed42cf979b6016b54890f6","deepnote_persisted_session":{"createdAt":"2022-12-01T13:48:05.991Z"},"deepnote_execution_queue":[]}}