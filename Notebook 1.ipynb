{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"cell_id":"b4e69e1e58f845d399236a7b7fac0c1b","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":4,"execution_start":1670504838978,"source_hash":"aa2a064f","tags":[]},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","from enum import Enum\n","import re\n","import random\n","import wandb\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":2,"metadata":{"cell_id":"d3e98de7febf436e8fa2c702468ca3ec","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":22761,"execution_start":1670427563731,"source_hash":"afeeeb3e","tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["wandb version 0.13.6 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.13.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/work/wandb/run-20221207_153945-1f5pokxe</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/atnlp/test-project/runs/1f5pokxe\" target=\"_blank\">dulcet-oath-1</a></strong> to <a href=\"https://wandb.ai/atnlp/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/atnlp/test-project/runs/1f5pokxe?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7f3ef9078670>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# Replace test-project by experiment\n","wandb.init(project=\"test-project\", entity=\"atnlp\")"]},{"cell_type":"code","execution_count":2,"metadata":{"cell_id":"c62d3cb2ff534269b1f3d30cdfbd98e3","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":4,"execution_start":1670503342846,"source_hash":"c303e84","tags":[]},"outputs":[],"source":["SOS_token = 0\n","EOS_token = 1\n","OOV_token = 2"]},{"cell_type":"markdown","metadata":{"cell_id":"32132a2df9284530897a1a10bba84f3d","deepnote_cell_type":"text-cell-h2","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":["## Dataloading"]},{"cell_type":"code","execution_count":4,"metadata":{"cell_id":"86dae37bd1814db895e26dd9b7873862","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":225,"execution_start":1670503342851,"source_hash":"5cbe3f70","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["fatal: destination path 'SCAN' already exists and is not an empty directory.\n"]}],"source":["# !git clone https://github.com/brendenlake/SCAN"]},{"cell_type":"code","execution_count":3,"metadata":{"cell_id":"6e429e976acf44a5ae9799bcc55352a7","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":0,"execution_start":1670503343091,"source_hash":"dd85ec7d","tags":[]},"outputs":[],"source":["class ScanSplit(Enum):\n","    SIMPLE_SPLIT = 'simple_split'\n","    LENGTH_SPLIT = 'length_split'\n","    FEW_SHOT_SPLIT = 'few_shot_split'\n","    ADD_PRIM_JUMP_SPLIT = 'add_prim_split'\n","    ADD_PRIM_TURNLEFT_SPLIT = 'add_prim_split' # shouldn't have same value => if condition won't happen"]},{"cell_type":"code","execution_count":4,"metadata":{"cell_id":"8ccb69a35933403ab2e0126888cae876","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":0,"execution_start":1670503343092,"source_hash":"bb1dc3ed","tags":[]},"outputs":[],"source":["class Lang:\n","    def __init__(self):\n","        self.word2index = {}\n","        self.word2count = {}\n","        self.index2word = {SOS_token: \"SOS\", EOS_token: \"EOS\", OOV_token: 'OOV'}\n","        self.n_words = len(self.index2word)  # Count tokens\n","\n","        self.max_length = 0\n","\n","    def add_sentence(self, sentence):\n","        \"\"\"Add sentence to vocab\"\"\"\n","        for word in sentence.split(' '):\n","            self._add_word(word)\n","\n","    def _add_word(self, word):\n","        \"\"\"Add word to vocab\"\"\"\n","        if word not in self.word2index:\n","            self.word2index[word] = self.n_words\n","            self.word2count[word] = 1\n","            self.index2word[self.n_words] = word\n","            self.n_words += 1\n","            self.max_length = max(len(word), self.max_length)\n","        else:\n","            self.word2count[word] += 1\n","\n","    def indexes_from_sentence(self, sentence: str):\n","        \"\"\"Get word ids from sentence\"\"\"\n","        indexes = [self.word2index.get(word,OOV_token) for word in sentence.split(' ')]\n","        return indexes\n","\n","    def tensor_from_sentence(self, sentence:str):\n","        \"\"\"Convert sentence to torch tensor\"\"\"\n","        indexes = self.indexes_from_sentence(sentence)\n","        return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)"]},{"cell_type":"code","execution_count":5,"metadata":{"cell_id":"f82e91c028134563878165a500aa61fd","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":6,"execution_start":1670503343093,"source_hash":"24e5f4b2","tags":[]},"outputs":[],"source":["class ScanDataset(Dataset):\n","    def __init__(self, split: ScanSplit, input_lang: Lang, output_lang: Lang, train: bool = True):\n","        \n","        self.input_lang = input_lang\n","        self.output_lang = output_lang\n","\n","\n","        self.X, self.y = self._get_data(split, train)\n","\n","\n","    def __len__(self):\n","        return len(self.y)\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n","\n","\n","    def convert_to_tensor(self, X, y):\n","        input_tensor = self.input_lang.tensor_from_sentence(X)\n","        target_tensor = self.output_lang.tensor_from_sentence(y)\n","        return (input_tensor, target_tensor)\n","    \n","\n","    def _get_data(self, split: ScanSplit, train: bool = True):\n","        \"\"\"Retrieve the right data for the selected split\"\"\"\n","        \n","        if split == ScanSplit.SIMPLE_SPLIT:\n","            X_train, y_train = self._extract_data_from_file('SCAN/simple_split/tasks_train_simple.txt')\n","            X_test, y_test = self._extract_data_from_file('SCAN/simple_split/tasks_test_simple.txt')\n","        elif split == ScanSplit.LENGTH_SPLIT:\n","            X_train, y_train = self._extract_data_from_file('SCAN/length_split/tasks_train_length.txt')\n","            X_test, y_test = self._extract_data_from_file('SCAN/length_split/tasks_test_length.txt')\n","        elif split == ScanSplit.ADD_PRIM_JUMP_SPLIT:\n","            X_train, y_train = self._extract_data_from_file('SCAN/add_prim_split/tasks_train_addprim_jump.txt')\n","            X_test, y_test = self._extract_data_from_file('SCAN/add_prim_split/tasks_test_addprim_jump.txt')\n","        elif split == ScanSplit.ADD_PRIM_TURNLEFT_SPLIT:\n","            X_train, y_train = self._extract_data_from_file('SCAN/add_prim_split/tasks_train_addprim_turn_left.txt')\n","            X_test, y_test = self._extract_data_from_file('SCAN/add_prim_split/tasks_test_addprim_turn_left.txt')\n","        else:\n","            raise Exception('Split not implemented')\n","        \n","        if train:\n","            X = X_train\n","            y = y_train\n","\n","            # Add words to vocabs\n","            for sen in X:\n","                self.input_lang.add_sentence(sen)\n","\n","            for sen in y:\n","                self.output_lang.add_sentence(sen)\n","        else:\n","            X = X_test\n","            y = y_test\n","\n","        return X,y\n","        \n","    def _extract_data_from_file(self, filepath: str):\n","        \"\"\"Get X and y from SCAN file\"\"\"\n","        with open(filepath) as f:\n","            txt_data = f.readlines()\n","\n","        # Format is in IN: ... OUT: ...\n","        lead_token = 'IN:'\n","        split_token = 'OUT:'\n","\n","        # Split at OUT and remove IN\n","        txt_data = [sen.strip(lead_token).split(split_token) for sen in txt_data]\n","\n","        in_txt = [sen[0] for sen in txt_data]\n","        out_txt = [sen[1] for sen in txt_data]\n","\n","        return in_txt, out_txt"]},{"cell_type":"code","execution_count":6,"metadata":{"cell_id":"cc2d3df152fb44318386c00221a1fcb3","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":3,"execution_start":1670503343101,"source_hash":"3c8963ba","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU not available, CPU used\n"]}],"source":["# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n","is_cuda = torch.cuda.is_available()\n","\n","# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n","if is_cuda:\n","    device = torch.device(\"cuda\")\n","    print(\"GPU is available\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"GPU not available, CPU used\")"]},{"cell_type":"code","execution_count":7,"metadata":{"cell_id":"365f2c2cd963413c8db18fcdb8d6d4e4","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":221,"execution_start":1670503343105,"source_hash":"e08742bc","tags":[]},"outputs":[],"source":["input_lang = Lang()\n","output_lang = Lang()\n","\n","train_dataset = ScanDataset(\n","    split=ScanSplit.SIMPLE_SPLIT,\n","    input_lang=input_lang,\n","    output_lang=output_lang,\n","    train=True\n",")\n","\n","test_dataset = ScanDataset(\n","    split=ScanSplit.SIMPLE_SPLIT,\n","    input_lang=input_lang,\n","    output_lang=output_lang,\n","    train=False\n",")\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n","\n","MAX_LENGTH = max(train_dataset.input_lang.max_length, train_dataset.output_lang.max_length)"]},{"cell_type":"markdown","metadata":{"cell_id":"281e4ae4d4b7471f91e10f4b9f2ec8f4","deepnote_cell_type":"text-cell-h2","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":["## Model"]},{"cell_type":"code","execution_count":8,"metadata":{"cell_id":"33011efe5de34b2b8a19c8befaa2d835","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":0,"execution_start":1670503347116,"source_hash":"495cf6ab","tags":[]},"outputs":[],"source":["def init_hidden(rnn_type, n_layers, hidden_size):\n","    if rnn_type == 'LSTM':\n","        return (\n","            torch.zeros(n_layers, 1, hidden_size, device=device),\n","            torch.zeros(n_layers, 1, hidden_size, device=device)\n","        )\n","    return torch.zeros(n_layers, 1, hidden_size, device=device)"]},{"cell_type":"code","execution_count":9,"metadata":{"cell_id":"86f81ce0fc0546beb8d8d291df73eb0b","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":2,"execution_start":1670503347765,"source_hash":"9d0372b3","tags":[]},"outputs":[],"source":["class EncoderRNN(nn.Module):\n","    def __init__(self, input_size, config):\n","        super(EncoderRNN, self).__init__()\n","        self.hidden_size = config['HIDDEN_SIZE']\n","        self.n_layers = config['N_LAYERS']\n","\n","        self.embedding = nn.Embedding(input_size, self.hidden_size)\n","\n","        self.dropout = nn.Dropout(config['DROPOUT'])\n","\n","        self.RNN_type = config['RNN_TYPE']\n","\n","        self.rnn = nn.__dict__[self.RNN_type](\n","            input_size=self.hidden_size,\n","            hidden_size=self.hidden_size,\n","            num_layers=self.n_layers,\n","            dropout=config['DROPOUT']\n","        )\n","\n","    def forward(self, encoder_input, hidden):\n","        output = self.embedding(encoder_input).view(1, 1, -1)\n","        output = self.dropout(output)\n","        output, hidden = self.rnn(output, hidden)\n","        return output, hidden\n","\n","    def init_hidden(self):\n","        return init_hidden(self.RNN_type, self.n_layers, self.hidden_size)"]},{"cell_type":"code","execution_count":261,"metadata":{"cell_id":"41de8cc24db84ccb9baf075d5ce51abf","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":6,"execution_start":1670503348495,"source_hash":"14f5e73b","tags":[]},"outputs":[],"source":["class DecoderRNN(nn.Module):\n","    def __init__(self, output_size, config):\n","        super(DecoderRNN, self).__init__()\n","        self.hidden_size = config['HIDDEN_SIZE']\n","        self.n_layers = config['N_LAYERS']\n","\n","        self.embedding = nn.Embedding(output_size, self.hidden_size)\n","\n","        self.dropout = nn.Dropout(config['DROPOUT'])\n","\n","        self.RNN_type = config['RNN_TYPE']\n","\n","        self.rnn = nn.__dict__[self.RNN_type](\n","            input_size=self.hidden_size,\n","            hidden_size=self.hidden_size,\n","            num_layers=self.n_layers,\n","            dropout=config['DROPOUT']\n","        )\n","\n","        self.out = nn.Linear(self.hidden_size*2, output_size)\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, decoder_input, hidden):\n","        output = self.embedding(decoder_input).view(1, 1, -1)\n","        output = self.dropout(output)\n","        output = F.relu(output)\n","        print(output.size(), hidden[0].size())\n","        output, hidden = self.rnn(output, hidden)\n","        output = self.softmax(self.out(output[0]))\n","        return output, hidden\n","\n","    def init_hidden(self):\n","        return init_hidden(self.RNN_type, self.n_layers, self.hidden_size)"]},{"cell_type":"code","execution_count":278,"metadata":{},"outputs":[],"source":["class AttnDecoderRNN(nn.Module):\n","    def __init__(self, output_size, config):\n","        super(AttnDecoderRNN, self).__init__()\n","        self.hidden_size = config['HIDDEN_SIZE']\n","        self.n_layers = config['N_LAYERS']\n","        self.output_size = output_size\n","        self.dropout = config['DROPOUT']\n","        self.RNN_type = config['RNN_TYPE']\n","        self.max_length = MAX_LENGTH\n","\n","        self.rnn = nn.__dict__[self.RNN_type](\n","            input_size=self.hidden_size,\n","            hidden_size=self.hidden_size*2,\n","            num_layers=self.n_layers,\n","            dropout=config['DROPOUT']\n","        )\n","        self.W = nn.Parameter(torch.randn((self.hidden_size, self.hidden_size)))\n","        self.U = nn.Parameter(torch.randn((self.hidden_size, self.hidden_size)))\n","        self.v = nn.Parameter(torch.randn((self.hidden_size, 1)))\n","\n","        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n","\n","        self.dropout = nn.Dropout(self.dropout)\n","\n","        self.out = nn.Linear(self.hidden_size*2, output_size)\n","\n","    def e(self, g, h):\n","        \"\"\"Computes the similarity between the previous decoder hidden state g and an encoder hidden state h\"\"\"\n","        # vT tanh(W g_(i-1) + U h_t)\n","        return self.v.T @ torch.tanh(self.W * g + self.U * h)\n","\n","    def alpha(self, encoder_hiddens, input_hidden, t):\n","        \"\"\"Computes the attention weight for a given encoder hidden state\"\"\"\n","        # alpha_it = exp(e(g_(i-1), h_t)) / sum(exp(e(g_(i-1), h_j)))\n","        T = len(encoder_hiddens)\n","        numerator = torch.exp(self.e(input_hidden, encoder_hiddens[t]))\n","\n","        denominator = 0\n","\n","        for j in range(T):\n","            denominator += torch.exp(self.e(input_hidden, encoder_hiddens[j]))\n","\n","        return numerator/denominator\n","\n","\n","    def forward(self, input, input_hidden, encoder_hiddens):\n","\n","        embedded = self.embedding(input).view(1, 1, -1)\n","        embedded = self.dropout(embedded)\n","\n","        # c_i = sum(alpha_it * h_t)\n","        c_i = 0\n","\n","        for t in range(len(encoder_hiddens)):\n","            alpha_it = self.alpha(encoder_hiddens, input_hidden, t)\n","            h_t = encoder_hiddens[t]\n","            c_i += alpha_it * h_t\n","\n","        hidden = torch.concat((input_hidden, c_i), dim=2) # Concatenate the context vector and the decoder hidden state\n","        output, hidden = self.rnn(embedded, hidden) \n","        output = F.log_softmax(self.out(output[0]), dim=0)\n","\n","        # Seperate the concatenated hidden state into the decoder hidden state and the context vector\n","        hidden, context = torch.split(hidden, self.hidden_size, dim=2)\n","\n","        return output, hidden"]},{"cell_type":"code","execution_count":285,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["6m 10s (- 55m 30s) (100 10%) 0.0000\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/var/folders/s3/51rzg94s5318dvd1cr9t6cq40000gn/T/ipykernel_6687/3317176120.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdecoder1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttnDecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtrain_iterations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/var/folders/s3/51rzg94s5318dvd1cr9t6cq40000gn/T/ipykernel_6687/1198936862.py\u001b[0m in \u001b[0;36mtrain_iterations\u001b[0;34m(encoder, decoder, n_iters, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mplot_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/var/folders/s3/51rzg94s5318dvd1cr9t6cq40000gn/T/ipykernel_6687/2689134144.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["hidden_size = 256\n","\n","config = {\n","    'HIDDEN_SIZE': 256, # 25, 50, 100, 200, or 400\n","    'RNN_TYPE': 'RNN', # RNN, GRU or LSTM\n","    'N_LAYERS': 2, # 1 or 2\n","    'DROPOUT': 0, # 0, 0.1 or 0.5\n","}\n","\n","wandb.config = config\n","\n","\n","encoder1 = EncoderRNN(train_dataset.input_lang.n_words, config).to(device)\n","# decoder1 = DecoderRNN(train_dataset.output_lang.n_words, config).to(device)\n","decoder1 = AttnDecoderRNN(train_dataset.output_lang.n_words, config).to(device)\n","\n","train_iterations(encoder1, decoder1, 1000, print_every=100)"]},{"cell_type":"code","execution_count":283,"metadata":{"cell_id":"d62e5697557c4fd2a20b7c8558f4f563","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":2,"execution_start":1670503350433,"source_hash":"eeee3fab","tags":[]},"outputs":[],"source":["teacher_forcing_ratio = .5\n","\n","\n","def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion,\n","          max_length=MAX_LENGTH):\n","    encoder_hidden = encoder.init_hidden()\n","\n","    encoder_optimizer.zero_grad()\n","    decoder_optimizer.zero_grad()\n","\n","    input_length = input_tensor.size(0)\n","    target_length = target_tensor.size(0)\n","\n","    encoder_hidden_all = torch.zeros(max_length, encoder.hidden_size, device=device)\n","\n","    loss = 0\n","\n","    for ei in range(input_length):\n","        encoder_output, encoder_hidden = encoder(\n","            input_tensor[ei], encoder_hidden)\n","        if encoder.RNN_type == 'LSTM':\n","            encoder_hidden_all[ei] = encoder_hidden[0][0, 0]\n","        else:\n","            encoder_hidden_all[ei] = encoder_hidden[0, 0]\n","\n","    decoder_input = torch.tensor([[SOS_token]], device=device)\n","\n","    decoder_hidden = encoder_hidden\n","\n","    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n","\n","    if use_teacher_forcing:\n","        # Teacher forcing: Feed the target as the next input\n","        for di in range(target_length):\n","            if decoder.__class__.__name__ == 'AttnDecoderRNN':\n","                decoder_output, decoder_hidden = decoder(\n","                    decoder_input, decoder_hidden, encoder_hidden_all)\n","            else:\n","                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n","                \n","\n","            loss += criterion(decoder_output, target_tensor[di])\n","            decoder_input = target_tensor[di]  # Teacher forcing\n","\n","    else:\n","        # Without teacher forcing: use its own predictions as the next input\n","        for di in range(target_length):\n","            if decoder.__class__.__name__ == 'AttnDecoderRNN':\n","                decoder_output, decoder_hidden = decoder(\n","                    decoder_input, decoder_hidden, encoder_hidden_all)\n","            else:\n","                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n","                \n","\n","            topv, topi = decoder_output.topk(1)\n","            decoder_input = topi.squeeze().detach()  # detach from history as input\n","\n","            loss += criterion(decoder_output, target_tensor[di])\n","            if decoder_input.item() == EOS_token:\n","                break\n","\n","    loss.backward()\n","\n","    encoder_optimizer.step()\n","    decoder_optimizer.step()\n","\n","    return loss.item() / target_length"]},{"cell_type":"code","execution_count":87,"metadata":{"cell_id":"3621122304714d388a2aa6ecf2eb9ed5","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":4,"execution_start":1670503351370,"source_hash":"73fe1d14","tags":[]},"outputs":[],"source":["import math\n","import time\n","\n","\n","def as_minutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","\n","def time_since(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"]},{"cell_type":"code","execution_count":88,"metadata":{"cell_id":"7997714cf263454882ec0f307eec7a01","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":4,"execution_start":1670503352158,"source_hash":"e6b242dc","tags":[]},"outputs":[],"source":["import matplotlib.pyplot as plt\n","plt.switch_backend('agg')\n","import matplotlib.ticker as ticker\n","import numpy as np\n","\n","\n","def show_plot(points):\n","    plt.figure()\n","    fig, ax = plt.subplots()\n","    loc = ticker.MultipleLocator(base=0.2)\n","    ax.yaxis.set_major_locator(loc)\n","    plt.plot(points)"]},{"cell_type":"code","execution_count":284,"metadata":{"cell_id":"163b98d7c7f14ab9bfb74d0a4ad0a5a2","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":2,"execution_start":1670503353455,"source_hash":"fe5cb239","tags":[]},"outputs":[],"source":["def train_iterations(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=1e-2):\n","    start = time.time()\n","    plot_losses = []\n","    print_loss_total = 0  # Reset every print_every\n","    plot_loss_total = 0  # Reset every plot_every\n","\n","    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n","    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n","    criterion = nn.NLLLoss()\n","\n","    for iteration in range(1, n_iters + 1):\n","        X, y = train_dataset[random.randrange(len(train_dataset))]\n","        input_tensor, target_tensor = train_dataset.convert_to_tensor(X, y)\n","\n","        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n","        print_loss_total += loss\n","        plot_loss_total += loss\n","\n","        if iteration % print_every == 0:\n","            print_loss_avg = print_loss_total / print_every\n","            print_loss_total = 0\n","            # wandb.log({\"avg_loss\": print_loss_avg})\n","            print('%s (%d %d%%) %.4f' % (time_since(start, iteration / n_iters),\n","                                         iteration, iteration / n_iters * 100, print_loss_avg))\n","\n","        if iteration % plot_every == 0:\n","            plot_loss_avg = plot_loss_total / plot_every\n","            plot_losses.append(plot_loss_avg)\n","            plot_loss_total = 0\n","\n","    show_plot(plot_losses)"]},{"cell_type":"code","execution_count":90,"metadata":{"cell_id":"36ba95c94a8f4c94830460bd0d134177","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":5,"execution_start":1670428490484,"source_hash":"7ef4a510","tags":[]},"outputs":[],"source":["def evaluate(dataset, encoder, decoder, num_layers, hidden_size, EOS_token, device, verbose=False, batch_size=1, shuffle=False):\n","    encoder.eval()\n","    decoder.eval()\n","\n","    accs = []\n","    \n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n","\n","    with torch.no_grad():\n","        for i, (x, y) in tqdm(enumerate(dataloader), total=len(dataloader), leave=False, desc=\"Evaluating\"):\n","            preds = []\n","\n","            x = x.squeeze()\n","            x = torch.cat((x, EOS_tensor), dim=0)\n","            x = x.to(device)\n","\n","            y = y.squeeze()\n","            y = torch.cat((y, EOS_tensor), dim=0)\n","            y = y.to(device)\n","\n","            encoder_hidden = torch.zeros(num_layers, 1, hidden_size, device=device)\n","            encoder_cx = torch.zeros((num_layers, 1, hidden_size), device=device)\n","\n","            decoder_input = torch.tensor([[SOS_token]], device=device)\n","            decoder_cx = torch.zeros((num_layers, 1, hidden_size), device=device)\n","\n","            for j in range(len(x)):\n","                _, encoder_hidden, encoder_cx = encoder(x[j], encoder_hidden, encoder_cx)\n","\n","            decoder_hidden = encoder_hidden\n","\n","            for j in range(len(y)):\n","                decoder_output, decoder_hidden, decoder_cx = decoder(\n","                    decoder_input, decoder_hidden, decoder_cx\n","                )\n","\n","                decoder_input = decoder_output.topk(1)[1]\n","\n","                preds.append(decoder_input.item())\n","\n","                if decoder_input.item() == EOS_token:\n","                    break\n","\n","            preds = np.array(preds)\n","            gts = y.detach().cpu().numpy()\n","            \n","            if len(preds) == len(gts):\n","                accs.append(np.all(preds == gts))\n","            else:\n","                accs.append(0)\n","          \n","    if verbose:\n","        print(\"Accuracy\", np.mean(accs))\n","    \n","    return np.mean(accs)"]},{"cell_type":"code","execution_count":91,"metadata":{"cell_id":"c6fad91cd002456b94e1d893ba7d87e2","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":2,"execution_start":1670503361672,"source_hash":"5274b8f9","tags":[]},"outputs":[],"source":["def evaluate(dataset, encoder, decoder, device=device, verbose=False, batch_size=1, shuffle=False, max_length=MAX_LENGTH):\n","    encoder.eval()\n","    decoder.eval()\n","\n","    accuracies = []\n","\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n","\n","    with torch.no_grad():\n","        for i, (sentence, truth) in tqdm(enumerate(dataloader), total=len(dataloader), leave=False, desc=\"Evaluating\"):\n","            preds = []\n","\n","            input_tensor, target_tensor = dataset.convert_to_tensor(sentence[0], truth[0])\n","            input_length = input_tensor.size()[0]\n","\n","            encoder_hidden = encoder.init_hidden()\n","            encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n","\n","            for ei in range(input_length):\n","                encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n","                encoder_outputs[ei] += encoder_output[0, 0]\n","\n","            decoder_input = torch.tensor([[SOS_token]], device=device)\n","            decoder_hidden = decoder.init_hidden()\n","            decoded_words = []\n","            decoder_attentions = torch.zeros(max_length, max_length)\n","\n","            for di in range(max_length):\n","                try:\n","                    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n","                except TypeError:\n","                    decoder_output, decoder_hidden, decoder_attention = decoder(\n","                        decoder_input, decoder_hidden, encoder_outputs)\n","\n","                    decoder_attentions[di - 1] = decoder_attention.data\n","\n","                topv, topi = decoder_output.data.topk(1)\n","                if topi.item() == EOS_token:\n","                    decoded_words.append('<EOS>')\n","                    break\n","                else:\n","                    decoded_words.append(dataset.output_lang.index2word[topi.item()])\n","\n","                # decoder_input = topi.squeeze().detach()\n","\n","                decoder_input = decoder_output.topk(1)[1]\n","                preds.append(decoder_input.item())\n","\n","                if decoder_input.item() == EOS_token:\n","                    break\n","\n","            preds = np.array(preds)\n","            gts = target_tensor.detach().cpu().numpy()\n","\n","            if len(preds) == len(gts):\n","                accuracies.append(np.all(preds == gts))\n","            else:\n","                accuracies.append(0)\n","\n","    if verbose:\n","        print(\"Accuracy\", np.mean(accuracies))\n","\n","    return np.mean(accuracies)"]},{"cell_type":"code","execution_count":92,"metadata":{"cell_id":"858d1c98ac4141238345c4ccc4c20672","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":2,"execution_start":1670503370941,"source_hash":"4fedd3c8","tags":[]},"outputs":[],"source":["def evaluate_random(encoder, decoder, n=10):\n","    for i in range(n):\n","        rand_idx = random.randrange(len(test_dataset))\n","        pair = test_dataset.X[rand_idx], test_dataset.y[rand_idx]\n","        output_words, attentions = evaluate(encoder, decoder, pair[0])\n","        output_sentence = ' '.join(output_words)\n","        print(output_sentence)"]},{"cell_type":"code","execution_count":118,"metadata":{"cell_id":"a49864cef1ec4e8d9dc3eef04e232090","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":592,"execution_start":1670428498511,"source_hash":"a430f441","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Encoder hidden: torch.Size([13, 256])\n"]},{"ename":"RuntimeError","evalue":"The expanded size of the tensor (13) must match the existing size (16) at non-singleton dimension 0.  Target sizes: [13].  Tensor sizes: [16]","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/var/folders/s3/51rzg94s5318dvd1cr9t6cq40000gn/T/ipykernel_6687/2187133800.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'encoder_hidden'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/var/folders/s3/51rzg94s5318dvd1cr9t6cq40000gn/T/ipykernel_6687/3336804050.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdecoder1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttnDecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtrain_iterations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/var/folders/s3/51rzg94s5318dvd1cr9t6cq40000gn/T/ipykernel_6687/1198936862.py\u001b[0m in \u001b[0;36mtrain_iterations\u001b[0;34m(encoder, decoder, n_iters, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mplot_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/var/folders/s3/51rzg94s5318dvd1cr9t6cq40000gn/T/ipykernel_6687/2187133800.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 decoder_output, decoder_hidden, decoder_attention = decoder(\n\u001b[0m\u001b[1;32m     51\u001b[0m                     decoder_input, decoder_hidden, encoder_hidden_all)\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/var/folders/s3/51rzg94s5318dvd1cr9t6cq40000gn/T/ipykernel_6687/102267913.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, decoder_input, hidden, encoder_hidden)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# vT tanh(W g_(i-1) + U h_t)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Encoder hidden: {encoder_hidden.size()}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Uh: {torch.matmul((self.U.expand(encoder_hidden.size(0)).t(), -1), encoder_hidden)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Wg: {self.W*hidden[0]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         e = torch.matmul(\n","\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (13) must match the existing size (16) at non-singleton dimension 0.  Target sizes: [13].  Tensor sizes: [16]"]}],"source":["hidden_size = 256\n","\n","config = {\n","    'HIDDEN_SIZE': 256, # 25, 50, 100, 200, or 400\n","    'RNN_TYPE': 'LSTM', # RNN, GRU or LSTM\n","    'N_LAYERS': 2, # 1 or 2\n","    'DROPOUT': 0, # 0, 0.1 or 0.5\n","}\n","\n","wandb.config = config\n","\n","\n","encoder1 = EncoderRNN(train_dataset.input_lang.n_words, config).to(device)\n","# decoder1 = DecoderRNN(train_dataset.output_lang.n_words, config).to(device)\n","decoder1 = AttnDecoderRNN(train_dataset.output_lang.n_words, config).to(device)\n","\n","train_iterations(encoder1, decoder1, 1000, print_every=100)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"4937655619624487a7aed6a056b48f70","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"source_hash":"b623e53d","tags":[]},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"cell_id":"173c017af69b464c9f2ecbdfd1774a22","deepnote_cell_type":"text-cell-h3","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":["### Experiment 1"]},{"cell_type":"markdown","metadata":{"cell_id":"55ce5211cbff4c5e8e900aded7380f89","deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":["The top-performing architecture was a LSTM with no attention, 2\n","layers of 200 hidden units, and no dropout. The best-overall\n","network achieved 99.7% correct."]},{"cell_type":"markdown","metadata":{"cell_id":"e5b250c9-18cf-47cd-9913-1791cf3a6eb3","deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":["SCAN tasks were randomly split into a training set (80%) and a test set (20%)."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"8b055f021e124ff7a02904bfcddda40b","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"source_hash":"b623e53d","tags":[]},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"cell_id":"7b78a617eb554d6b9f9c3d5f9bf38cd8","deepnote_cell_type":"text-cell-h3","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":["### Experiment 2"]},{"cell_type":"markdown","metadata":{"cell_id":"6810bb7d3b0c49eba78f38d3a8753986","deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":["The best result (20.8% on average, again over 5 runs) is achieved\n","by a GRU with attention, one 50-dimensional hidden layer,\n","and dropout 0.5"]},{"cell_type":"code","execution_count":35,"metadata":{"cell_id":"ee5ff92d4ca94a79aa878f58f62a34c7","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":6333,"execution_start":1670504921436,"source_hash":"4493d43e","tags":[]},"outputs":[{"data":{"text/html":["Finishing last run (ID:14zocxrl) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_loss</td><td>█▆▆▆▅▅▅▄▄▄▄▄▄▃▃▂▃▂▃▃▃▂▂▂▂▂▂▂▁▁▂▂▁▁▂▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_loss</td><td>0.9095</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">brisk-pine-5</strong>: <a href=\"https://wandb.ai/atnlp/experiment-2/runs/14zocxrl\" target=\"_blank\">https://wandb.ai/atnlp/experiment-2/runs/14zocxrl</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20221208_124413-14zocxrl/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:14zocxrl). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["wandb version 0.13.6 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.13.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/work/wandb/run-20221208_130841-4giz74tb</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/atnlp/experiment-2/runs/4giz74tb\" target=\"_blank\">cool-disco-6</a></strong> to <a href=\"https://wandb.ai/atnlp/experiment-2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/atnlp/experiment-2/runs/4giz74tb?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7fcf16b197f0>"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["wandb.init(project=\"experiment-2\", entity=\"atnlp\")"]},{"cell_type":"code","execution_count":36,"metadata":{"cell_id":"aac03ce487824b95b8858ca8957bfac3","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":1,"execution_start":1670504927771,"source_hash":"49b6a13c","tags":[]},"outputs":[],"source":["input_lang = Lang()\n","output_lang = Lang()"]},{"cell_type":"code","execution_count":37,"metadata":{"cell_id":"488fdc94c8854fe7b3aefd2eb3b033c8","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":168,"execution_start":1670504927773,"source_hash":"6bb56092","tags":[]},"outputs":[],"source":["train_dataset = ScanDataset(\n","    split=ScanSplit.LENGTH_SPLIT,\n","    input_lang=input_lang,\n","    output_lang=output_lang,\n","    train=True\n",")\n","\n","test_dataset = ScanDataset(\n","    split=ScanSplit.LENGTH_SPLIT,\n","    input_lang=input_lang,\n","    output_lang=output_lang,\n","    train=False\n",")\n","\n","assert (len(train_dataset) == 16990)\n","assert (len(test_dataset) == 3920)"]},{"cell_type":"code","execution_count":38,"metadata":{"cell_id":"3143f6fa0b99463fba724730c39666a7","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":3,"execution_start":1670504930131,"source_hash":"cd3df8c1","tags":[]},"outputs":[],"source":["experiment_2_config = dict(HIDDEN_SIZE=50, N_LAYERS=1, DROPOUT=.5, RNN_TYPE='GRU')\n","overall_best_config = dict(HIDDEN_SIZE=200, N_LAYERS=2, DROPOUT=.5, RNN_TYPE='LSTM')\n","\n","config = experiment_2_config"]},{"cell_type":"code","execution_count":39,"metadata":{"cell_id":"1c229374e5574fc79c8ac726489a6d08","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":6,"execution_start":1670504931413,"source_hash":"f8dd64c8","tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/shared-libs/python3.9/py/lib/python3.9/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]}],"source":["encoder_exp_2 = EncoderRNN(input_lang.n_words, config=config).to(device)\n","decoder_exp_2 = DecoderRNN(output_lang.n_words, config=config).to(device)\n","attn_decoder_exp_2 = AttnDecoderRNN(output_lang.n_words, config=config).to(device)"]},{"cell_type":"code","execution_count":40,"metadata":{"cell_id":"70fb46eddada431a9e528181fa9d602d","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":491853,"execution_start":1670504938824,"source_hash":"5b1bc7dd","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["0m 4s (- 7m 30s) (100 1%) 2.1491\n","0m 9s (- 7m 53s) (200 2%) 1.8391\n","0m 14s (- 7m 40s) (300 3%) 1.8041\n","0m 19s (- 7m 37s) (400 4%) 1.7459\n","0m 24s (- 7m 37s) (500 5%) 1.7441\n","0m 28s (- 7m 27s) (600 6%) 1.7132\n","0m 33s (- 7m 20s) (700 7%) 1.7615\n","0m 38s (- 7m 19s) (800 8%) 1.6810\n","0m 43s (- 7m 18s) (900 9%) 1.6580\n","0m 48s (- 7m 13s) (1000 10%) 1.6702\n","0m 52s (- 7m 6s) (1100 11%) 1.6138\n","0m 57s (- 7m 0s) (1200 12%) 1.4693\n","1m 2s (- 6m 56s) (1300 13%) 1.5826\n","1m 7s (- 6m 53s) (1400 14%) 1.5038\n","1m 12s (- 6m 52s) (1500 15%) 1.5233\n","1m 18s (- 6m 49s) (1600 16%) 1.3632\n","1m 23s (- 6m 46s) (1700 17%) 1.4715\n","1m 28s (- 6m 43s) (1800 18%) 1.4320\n","1m 34s (- 6m 40s) (1900 19%) 1.4505\n","1m 39s (- 6m 36s) (2000 20%) 1.4093\n","1m 44s (- 6m 32s) (2100 21%) 1.3209\n","1m 49s (- 6m 26s) (2200 22%) 1.3297\n","1m 54s (- 6m 21s) (2300 23%) 1.3695\n","1m 58s (- 6m 15s) (2400 24%) 1.3888\n","2m 3s (- 6m 10s) (2500 25%) 1.2416\n","2m 8s (- 6m 7s) (2600 26%) 1.3154\n","2m 14s (- 6m 3s) (2700 27%) 1.3073\n","2m 20s (- 6m 0s) (2800 28%) 1.2706\n","2m 25s (- 5m 55s) (2900 28%) 1.2736\n","2m 29s (- 5m 49s) (3000 30%) 1.1818\n","2m 34s (- 5m 44s) (3100 31%) 1.2034\n","2m 39s (- 5m 39s) (3200 32%) 1.2313\n","2m 44s (- 5m 33s) (3300 33%) 1.1804\n","2m 49s (- 5m 28s) (3400 34%) 1.1946\n","2m 54s (- 5m 23s) (3500 35%) 1.2571\n","2m 58s (- 5m 17s) (3600 36%) 1.1272\n","3m 3s (- 5m 11s) (3700 37%) 1.2691\n","3m 7s (- 5m 6s) (3800 38%) 1.1276\n","3m 12s (- 5m 0s) (3900 39%) 1.0795\n","3m 17s (- 4m 55s) (4000 40%) 1.1970\n","3m 21s (- 4m 50s) (4100 41%) 1.0799\n","3m 26s (- 4m 45s) (4200 42%) 1.1466\n","3m 31s (- 4m 40s) (4300 43%) 1.1299\n","3m 36s (- 4m 35s) (4400 44%) 1.1110\n","3m 41s (- 4m 30s) (4500 45%) 1.0664\n","3m 45s (- 4m 25s) (4600 46%) 1.1488\n","3m 50s (- 4m 20s) (4700 47%) 1.0704\n","3m 55s (- 4m 15s) (4800 48%) 1.0589\n","4m 1s (- 4m 11s) (4900 49%) 1.1836\n","4m 6s (- 4m 6s) (5000 50%) 1.0840\n","4m 11s (- 4m 1s) (5100 51%) 1.0518\n","4m 16s (- 3m 56s) (5200 52%) 1.0239\n","4m 21s (- 3m 51s) (5300 53%) 1.1148\n","4m 25s (- 3m 46s) (5400 54%) 1.0871\n","4m 30s (- 3m 41s) (5500 55%) 1.0094\n","4m 35s (- 3m 36s) (5600 56%) 1.0700\n","4m 40s (- 3m 31s) (5700 56%) 0.9588\n","4m 44s (- 3m 26s) (5800 57%) 0.9640\n","4m 49s (- 3m 21s) (5900 59%) 0.9953\n","4m 54s (- 3m 16s) (6000 60%) 0.9529\n","4m 58s (- 3m 11s) (6100 61%) 1.0720\n","5m 3s (- 3m 6s) (6200 62%) 1.0522\n","5m 8s (- 3m 1s) (6300 63%) 0.9502\n","5m 13s (- 2m 56s) (6400 64%) 0.9330\n","5m 18s (- 2m 51s) (6500 65%) 1.0032\n","5m 22s (- 2m 46s) (6600 66%) 0.8894\n","5m 27s (- 2m 41s) (6700 67%) 0.9124\n","5m 32s (- 2m 36s) (6800 68%) 0.9259\n","5m 37s (- 2m 31s) (6900 69%) 0.9369\n","5m 42s (- 2m 26s) (7000 70%) 0.9348\n","5m 46s (- 2m 21s) (7100 71%) 0.9371\n","5m 51s (- 2m 16s) (7200 72%) 0.8928\n","5m 56s (- 2m 11s) (7300 73%) 0.8600\n","6m 1s (- 2m 7s) (7400 74%) 0.9569\n","6m 7s (- 2m 2s) (7500 75%) 0.9180\n","6m 11s (- 1m 57s) (7600 76%) 0.8699\n","6m 16s (- 1m 52s) (7700 77%) 0.8710\n","6m 20s (- 1m 47s) (7800 78%) 0.9819\n","6m 25s (- 1m 42s) (7900 79%) 0.9201\n","6m 30s (- 1m 37s) (8000 80%) 0.7852\n","6m 35s (- 1m 32s) (8100 81%) 0.8309\n","6m 40s (- 1m 27s) (8200 82%) 0.8153\n","6m 44s (- 1m 22s) (8300 83%) 0.8456\n","6m 49s (- 1m 18s) (8400 84%) 0.8527\n","6m 54s (- 1m 13s) (8500 85%) 0.8404\n","6m 58s (- 1m 8s) (8600 86%) 0.8217\n","7m 3s (- 1m 3s) (8700 87%) 0.8000\n","7m 8s (- 0m 58s) (8800 88%) 0.7513\n","7m 13s (- 0m 53s) (8900 89%) 0.7096\n","7m 18s (- 0m 48s) (9000 90%) 0.7082\n","7m 23s (- 0m 43s) (9100 91%) 0.7636\n","7m 29s (- 0m 39s) (9200 92%) 0.7665\n","7m 35s (- 0m 34s) (9300 93%) 0.7916\n","7m 41s (- 0m 29s) (9400 94%) 0.7543\n","7m 46s (- 0m 24s) (9500 95%) 0.6984\n","7m 51s (- 0m 19s) (9600 96%) 0.6854\n","7m 56s (- 0m 14s) (9700 97%) 0.7542\n","8m 1s (- 0m 9s) (9800 98%) 0.7373\n","8m 6s (- 0m 4s) (9900 99%) 0.7100\n","8m 11s (- 0m 0s) (10000 100%) 0.7140\n"]}],"source":["train_iterations(encoder_exp_2, attn_decoder_exp_2, 10000, print_every=100)"]},{"cell_type":"code","execution_count":41,"metadata":{"cell_id":"825c7142088549c5b524881463b8eb34","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":13478,"execution_start":1670505430680,"source_hash":"e13b4d80","tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":[]},{"data":{"text/plain":["0.0"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["evaluate(test_dataset, encoder_exp_2, attn_decoder_exp_2)"]},{"cell_type":"markdown","metadata":{"cell_id":"58d77281f8ff46aa8124835bd73212a9","deepnote_cell_type":"text-cell-h3","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":["### Experiment 3"]},{"cell_type":"markdown","metadata":{"cell_id":"2db6686e0951479c9500c5a1d1fb7d89","deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":["The best performance is achieved by\n","a GRU network with attention, one layer with 100 hidden\n","units, and dropout of 0.1 (90.3% accuracy). "]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"2bfbc1a109ab460a81c8e56a56db610d","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"source_hash":"b623e53d","tags":[]},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown","tags":[]},"source":["<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=ec00d141-8917-4313-a10a-78395d2ec852' target=\"_blank\">\n","<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n","Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"]}],"metadata":{"deepnote":{},"deepnote_execution_queue":[],"deepnote_notebook_id":"aa078de21384434e9243e98f7929ffe5","deepnote_persisted_session":{"createdAt":"2022-12-08T13:47:47.198Z"},"kernelspec":{"display_name":"Python 3.9.15 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15"},"orig_nbformat":2,"vscode":{"interpreter":{"hash":"a665b5d41d17b532ea9890333293a1b812fa0b73c9c25c950b3cedf1bebd0438"}}},"nbformat":4,"nbformat_minor":0}
